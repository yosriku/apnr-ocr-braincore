{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P57SeU1bFTse"
      },
      "source": [
        "Download Model for fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWTze9RFTsg",
        "outputId": "eaade5f3-80e7-4dd8-feab-c0516f367ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-03 15:36:49--  https://github.com/JaidedAI/EasyOCR/releases/download/v1.3/english_g2.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/247266215/adab9d00-8a64-11eb-8f75-0958f4d0892d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241003%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241003T153649Z&X-Amz-Expires=300&X-Amz-Signature=9e3fd0839ce816fe71cb83d3d8ca8975a2b3c9b9b124415416e54e359217f945&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Denglish_g2.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-10-03 15:36:49--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/247266215/adab9d00-8a64-11eb-8f75-0958f4d0892d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241003%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241003T153649Z&X-Amz-Expires=300&X-Amz-Signature=9e3fd0839ce816fe71cb83d3d8ca8975a2b3c9b9b124415416e54e359217f945&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Denglish_g2.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14040947 (13M) [application/octet-stream]\n",
            "Saving to: ‘english_g2.zip’\n",
            "\n",
            "english_g2.zip      100%[===================>]  13.39M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-10-03 15:36:50 (136 MB/s) - ‘english_g2.zip’ saved [14040947/14040947]\n",
            "\n",
            "Archive:  english_g2.zip\n",
            "replace english_g2.pth? [y]es, [n]o, [A]ll, [N]one, [r]ename: mv: cannot move 'english_g2.pth' to 'saved_models/': Not a directory\n"
          ]
        }
      ],
      "source": [
        "# Download model englis_g2\n",
        "!wget https://github.com/JaidedAI/EasyOCR/releases/download/v1.3/english_g2.zip\n",
        "!unzip english_g2.zip\n",
        "!mv english_g2.pth saved_models/\n",
        "!rm english_g2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpM2cjqrFTsh"
      },
      "source": [
        "Prepare dataset training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmtPM93lFTsh"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2d6kb0BWlqO",
        "outputId": "e26035db-793e-4d56-e027-52c8caaa6c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "en_sample.zip\n",
            "en_sample\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Navigate to the folder where the zip file is stored\n",
        "!ls /content/drive/MyDrive/apnr\n",
        "\n",
        "# Step 3: Unzip the file and move it to the content/all_data folder\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Unzipping the file (replace 'yourfile.zip' with the actual zip file name)\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/apnr/en_sample.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/all_data')  # Extracting to content/all_data\n",
        "\n",
        "# Optional: Verify the extraction\n",
        "!ls /content/all_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yqw_4Jg6FTsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a70ac9-28c5-4a35-b07d-5726a1caf61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install lmdb pillow nltk natsort pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M0R-P4owFTsi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "import yaml\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8m7UqH0iFTsi"
      },
      "outputs": [],
      "source": [
        "cudnn.benchmark = True\n",
        "cudnn.deterministic = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zs16fEXHSggw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class TPS_SpatialTransformerNetwork(nn.Module):\n",
        "    \"\"\" Rectification Network of RARE, namely TPS based STN \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_size, I_r_size, I_channel_num=1):\n",
        "        \"\"\" Based on RARE TPS\n",
        "        input:\n",
        "            batch_I: Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
        "            I_size : (height, width) of the input image I\n",
        "            I_r_size : (height, width) of the rectified image I_r\n",
        "            I_channel_num : the number of channels of the input image I\n",
        "        output:\n",
        "            batch_I_r: rectified image [batch_size x I_channel_num x I_r_height x I_r_width]\n",
        "        \"\"\"\n",
        "        super(TPS_SpatialTransformerNetwork, self).__init__()\n",
        "        self.F = F\n",
        "        self.I_size = I_size\n",
        "        self.I_r_size = I_r_size  # = (I_r_height, I_r_width)\n",
        "        self.I_channel_num = I_channel_num\n",
        "        self.LocalizationNetwork = LocalizationNetwork(self.F, self.I_channel_num)\n",
        "        self.GridGenerator = GridGenerator(self.F, self.I_r_size)\n",
        "\n",
        "    def forward(self, batch_I):\n",
        "        batch_C_prime = self.LocalizationNetwork(batch_I)  # batch_size x K x 2\n",
        "        build_P_prime = self.GridGenerator.build_P_prime(batch_C_prime)  # batch_size x n (= I_r_width x I_r_height) x 2\n",
        "        build_P_prime_reshape = build_P_prime.reshape([build_P_prime.size(0), self.I_r_size[0], self.I_r_size[1], 2])\n",
        "        batch_I_r = F.grid_sample(batch_I, build_P_prime_reshape, padding_mode='border')\n",
        "\n",
        "        return batch_I_r\n",
        "\n",
        "\n",
        "class LocalizationNetwork(nn.Module):\n",
        "    \"\"\" Localization Network of RARE, which predicts C' (K x 2) from I (I_width x I_height) \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_channel_num):\n",
        "        super(LocalizationNetwork, self).__init__()\n",
        "        self.F = F\n",
        "        self.I_channel_num = I_channel_num\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.I_channel_num, out_channels=64, kernel_size=3, stride=1, padding=1,\n",
        "                      bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 64 x I_height/2 x I_width/2\n",
        "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 128 x I_height/4 x I_width/4\n",
        "            nn.Conv2d(128, 256, 3, 1, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # batch_size x 256 x I_height/8 x I_width/8\n",
        "            nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n",
        "            nn.AdaptiveAvgPool2d(1)  # batch_size x 512\n",
        "        )\n",
        "\n",
        "        self.localization_fc1 = nn.Sequential(nn.Linear(512, 256), nn.ReLU(True))\n",
        "        self.localization_fc2 = nn.Linear(256, self.F * 2)\n",
        "\n",
        "        # Init fc2 in LocalizationNetwork\n",
        "        self.localization_fc2.weight.data.fill_(0)\n",
        "        \"\"\" see RARE paper Fig. 6 (a) \"\"\"\n",
        "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
        "        ctrl_pts_y_top = np.linspace(0.0, -1.0, num=int(F / 2))\n",
        "        ctrl_pts_y_bottom = np.linspace(1.0, 0.0, num=int(F / 2))\n",
        "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
        "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
        "        initial_bias = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
        "        self.localization_fc2.bias.data = torch.from_numpy(initial_bias).float().view(-1)\n",
        "\n",
        "    def forward(self, batch_I):\n",
        "        \"\"\"\n",
        "        input:     batch_I : Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
        "        output:    batch_C_prime : Predicted coordinates of fiducial points for input batch [batch_size x F x 2]\n",
        "        \"\"\"\n",
        "        batch_size = batch_I.size(0)\n",
        "        features = self.conv(batch_I).view(batch_size, -1)\n",
        "        batch_C_prime = self.localization_fc2(self.localization_fc1(features)).view(batch_size, self.F, 2)\n",
        "        return batch_C_prime\n",
        "\n",
        "\n",
        "class GridGenerator(nn.Module):\n",
        "    \"\"\" Grid Generator of RARE, which produces P_prime by multiplying T with P \"\"\"\n",
        "\n",
        "    def __init__(self, F, I_r_size):\n",
        "        \"\"\" Generate P_hat and inv_delta_C for later \"\"\"\n",
        "        super(GridGenerator, self).__init__()\n",
        "        self.eps = 1e-6\n",
        "        self.I_r_height, self.I_r_width = I_r_size\n",
        "        self.F = F\n",
        "        self.C = self._build_C(self.F)  # F x 2\n",
        "        self.P = self._build_P(self.I_r_width, self.I_r_height)\n",
        "        ## for multi-gpu, you need register buffer\n",
        "        self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())  # F+3 x F+3\n",
        "        self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())  # n x F+3\n",
        "        ## for fine-tuning with different image width, you may use below instead of self.register_buffer\n",
        "        #self.inv_delta_C = torch.tensor(self._build_inv_delta_C(self.F, self.C)).float().cuda()  # F+3 x F+3\n",
        "        #self.P_hat = torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float().cuda()  # n x F+3\n",
        "\n",
        "    def _build_C(self, F):\n",
        "        \"\"\" Return coordinates of fiducial points in I_r; C \"\"\"\n",
        "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
        "        ctrl_pts_y_top = -1 * np.ones(int(F / 2))\n",
        "        ctrl_pts_y_bottom = np.ones(int(F / 2))\n",
        "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
        "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
        "        C = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
        "        return C  # F x 2\n",
        "\n",
        "    def _build_inv_delta_C(self, F, C):\n",
        "        \"\"\" Return inv_delta_C which is needed to calculate T \"\"\"\n",
        "        hat_C = np.zeros((F, F), dtype=float)  # F x F\n",
        "        for i in range(0, F):\n",
        "            for j in range(i, F):\n",
        "                r = np.linalg.norm(C[i] - C[j])\n",
        "                hat_C[i, j] = r\n",
        "                hat_C[j, i] = r\n",
        "        np.fill_diagonal(hat_C, 1)\n",
        "        hat_C = (hat_C ** 2) * np.log(hat_C)\n",
        "        # print(C.shape, hat_C.shape)\n",
        "        delta_C = np.concatenate(  # F+3 x F+3\n",
        "            [\n",
        "                np.concatenate([np.ones((F, 1)), C, hat_C], axis=1),  # F x F+3\n",
        "                np.concatenate([np.zeros((2, 3)), np.transpose(C)], axis=1),  # 2 x F+3\n",
        "                np.concatenate([np.zeros((1, 3)), np.ones((1, F))], axis=1)  # 1 x F+3\n",
        "            ],\n",
        "            axis=0\n",
        "        )\n",
        "        inv_delta_C = np.linalg.inv(delta_C)\n",
        "        return inv_delta_C  # F+3 x F+3\n",
        "\n",
        "    def _build_P(self, I_r_width, I_r_height):\n",
        "        I_r_grid_x = (np.arange(-I_r_width, I_r_width, 2) + 1.0) / I_r_width  # self.I_r_width\n",
        "        I_r_grid_y = (np.arange(-I_r_height, I_r_height, 2) + 1.0) / I_r_height  # self.I_r_height\n",
        "        P = np.stack(  # self.I_r_width x self.I_r_height x 2\n",
        "            np.meshgrid(I_r_grid_x, I_r_grid_y),\n",
        "            axis=2\n",
        "        )\n",
        "        return P.reshape([-1, 2])  # n (= self.I_r_width x self.I_r_height) x 2\n",
        "\n",
        "    def _build_P_hat(self, F, C, P):\n",
        "        n = P.shape[0]  # n (= self.I_r_width x self.I_r_height)\n",
        "        P_tile = np.tile(np.expand_dims(P, axis=1), (1, F, 1))  # n x 2 -> n x 1 x 2 -> n x F x 2\n",
        "        C_tile = np.expand_dims(C, axis=0)  # 1 x F x 2\n",
        "        P_diff = P_tile - C_tile  # n x F x 2\n",
        "        rbf_norm = np.linalg.norm(P_diff, ord=2, axis=2, keepdims=False)  # n x F\n",
        "        rbf = np.multiply(np.square(rbf_norm), np.log(rbf_norm + self.eps))  # n x F\n",
        "        P_hat = np.concatenate([np.ones((n, 1)), P, rbf], axis=1)\n",
        "        return P_hat  # n x F+3\n",
        "\n",
        "    def build_P_prime(self, batch_C_prime):\n",
        "        \"\"\" Generate Grid from batch_C_prime [batch_size x F x 2] \"\"\"\n",
        "        batch_size = batch_C_prime.size(0)\n",
        "        batch_inv_delta_C = self.inv_delta_C.repeat(batch_size, 1, 1)\n",
        "        batch_P_hat = self.P_hat.repeat(batch_size, 1, 1)\n",
        "        batch_C_prime_with_zeros = torch.cat((batch_C_prime, torch.zeros(\n",
        "            batch_size, 3, 2).float().to(device)), dim=1)  # batch_size x F+3 x 2\n",
        "        batch_T = torch.bmm(batch_inv_delta_C, batch_C_prime_with_zeros)  # batch_size x F+3 x 2\n",
        "        batch_P_prime = torch.bmm(batch_P_hat, batch_T)  # batch_size x n x 2\n",
        "        return batch_P_prime  # batch_size x n x 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q5uGzKHSSopr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        input : visual feature [batch_size x T x input_size]\n",
        "        output : contextual feature [batch_size x T x output_size]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.rnn.flatten_parameters()\n",
        "        except:\n",
        "            pass\n",
        "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
        "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oywBg7d2SrKH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_cell = AttentionCell(input_size, hidden_size, num_classes)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.generator = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def _char_to_onehot(self, input_char, onehot_dim=38):\n",
        "        input_char = input_char.unsqueeze(1)\n",
        "        batch_size = input_char.size(0)\n",
        "        one_hot = torch.FloatTensor(batch_size, onehot_dim).zero_().to(device)\n",
        "        one_hot = one_hot.scatter_(1, input_char, 1)\n",
        "        return one_hot\n",
        "\n",
        "    def forward(self, batch_H, text, is_train=True, batch_max_length=25):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            batch_H : contextual_feature H = hidden state of encoder. [batch_size x num_steps x num_classes]\n",
        "            text : the text-index of each image. [batch_size x (max_length+1)]. +1 for [GO] token. text[:, 0] = [GO].\n",
        "        output: probability distribution at each step [batch_size x num_steps x num_classes]\n",
        "        \"\"\"\n",
        "        batch_size = batch_H.size(0)\n",
        "        num_steps = batch_max_length + 1  # +1 for [s] at end of sentence.\n",
        "\n",
        "        output_hiddens = torch.FloatTensor(batch_size, num_steps, self.hidden_size).fill_(0).to(device)\n",
        "        hidden = (torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device),\n",
        "                  torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device))\n",
        "\n",
        "        if is_train:\n",
        "            for i in range(num_steps):\n",
        "                # one-hot vectors for a i-th char. in a batch\n",
        "                char_onehots = self._char_to_onehot(text[:, i], onehot_dim=self.num_classes)\n",
        "                # hidden : decoder's hidden s_{t-1}, batch_H : encoder's hidden H, char_onehots : one-hot(y_{t-1})\n",
        "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
        "                output_hiddens[:, i, :] = hidden[0]  # LSTM hidden index (0: hidden, 1: Cell)\n",
        "            probs = self.generator(output_hiddens)\n",
        "\n",
        "        else:\n",
        "            targets = torch.LongTensor(batch_size).fill_(0).to(device)  # [GO] token\n",
        "            probs = torch.FloatTensor(batch_size, num_steps, self.num_classes).fill_(0).to(device)\n",
        "\n",
        "            for i in range(num_steps):\n",
        "                char_onehots = self._char_to_onehot(targets, onehot_dim=self.num_classes)\n",
        "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
        "                probs_step = self.generator(hidden[0])\n",
        "                probs[:, i, :] = probs_step\n",
        "                _, next_input = probs_step.max(1)\n",
        "                targets = next_input\n",
        "\n",
        "        return probs  # batch_size x num_steps x num_classes\n",
        "\n",
        "\n",
        "class AttentionCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_embeddings):\n",
        "        super(AttentionCell, self).__init__()\n",
        "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)  # either i2i or h2h should have bias\n",
        "        self.score = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.rnn = nn.LSTMCell(input_size + num_embeddings, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, prev_hidden, batch_H, char_onehots):\n",
        "        # [batch_size x num_encoder_step x num_channel] -> [batch_size x num_encoder_step x hidden_size]\n",
        "        batch_H_proj = self.i2h(batch_H)\n",
        "        prev_hidden_proj = self.h2h(prev_hidden[0]).unsqueeze(1)\n",
        "        e = self.score(torch.tanh(batch_H_proj + prev_hidden_proj))  # batch_size x num_encoder_step * 1\n",
        "\n",
        "        alpha = F.softmax(e, dim=1)\n",
        "        context = torch.bmm(alpha.permute(0, 2, 1), batch_H).squeeze(1)  # batch_size x num_channel\n",
        "        concat_context = torch.cat([context, char_onehots], 1)  # batch_size x (num_channel + num_embedding)\n",
        "        cur_hidden = self.rnn(concat_context, prev_hidden)\n",
        "        return cur_hidden, alpha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Kc6s_H5WStj-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class VGG_FeatureExtractor(nn.Module):\n",
        "    \"\"\" FeatureExtractor of CRNN (https://arxiv.org/pdf/1507.05717.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(VGG_FeatureExtractor, self).__init__()\n",
        "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
        "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
        "        self.ConvNet = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 64x16x50\n",
        "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 128x8x25\n",
        "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),  # 256x8x25\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),  # 256x4x25\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),  # 512x4x25\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),  # 512x2x25\n",
        "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))  # 512x1x24\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "\n",
        "class RCNN_FeatureExtractor(nn.Module):\n",
        "    \"\"\" FeatureExtractor of GRCNN (https://papers.nips.cc/paper/6637-gated-recurrent-convolution-neural-network-for-ocr.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(RCNN_FeatureExtractor, self).__init__()\n",
        "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
        "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
        "        self.ConvNet = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # 64 x 16 x 50\n",
        "            GRCL(self.output_channel[0], self.output_channel[0], num_iteration=5, kernel_size=3, pad=1),\n",
        "            nn.MaxPool2d(2, 2),  # 64 x 8 x 25\n",
        "            GRCL(self.output_channel[0], self.output_channel[1], num_iteration=5, kernel_size=3, pad=1),\n",
        "            nn.MaxPool2d(2, (2, 1), (0, 1)),  # 128 x 4 x 26\n",
        "            GRCL(self.output_channel[1], self.output_channel[2], num_iteration=5, kernel_size=3, pad=1),\n",
        "            nn.MaxPool2d(2, (2, 1), (0, 1)),  # 256 x 2 x 27\n",
        "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 2, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True))  # 512 x 1 x 26\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "\n",
        "class ResNet_FeatureExtractor(nn.Module):\n",
        "    \"\"\" FeatureExtractor of FAN (http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf) \"\"\"\n",
        "\n",
        "    def __init__(self, input_channel, output_channel=512):\n",
        "        super(ResNet_FeatureExtractor, self).__init__()\n",
        "        self.ConvNet = ResNet(input_channel, output_channel, BasicBlock, [1, 2, 5, 3])\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ConvNet(input)\n",
        "\n",
        "\n",
        "# For Gated RCNN\n",
        "class GRCL(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, num_iteration, kernel_size, pad):\n",
        "        super(GRCL, self).__init__()\n",
        "        self.wgf_u = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=False)\n",
        "        self.wgr_x = nn.Conv2d(output_channel, output_channel, 1, 1, 0, bias=False)\n",
        "        self.wf_u = nn.Conv2d(input_channel, output_channel, kernel_size, 1, pad, bias=False)\n",
        "        self.wr_x = nn.Conv2d(output_channel, output_channel, kernel_size, 1, pad, bias=False)\n",
        "\n",
        "        self.BN_x_init = nn.BatchNorm2d(output_channel)\n",
        "\n",
        "        self.num_iteration = num_iteration\n",
        "        self.GRCL = [GRCL_unit(output_channel) for _ in range(num_iteration)]\n",
        "        self.GRCL = nn.Sequential(*self.GRCL)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\" The input of GRCL is consistant over time t, which is denoted by u(0)\n",
        "        thus wgf_u / wf_u is also consistant over time t.\n",
        "        \"\"\"\n",
        "        wgf_u = self.wgf_u(input)\n",
        "        wf_u = self.wf_u(input)\n",
        "        x = F.relu(self.BN_x_init(wf_u))\n",
        "\n",
        "        for i in range(self.num_iteration):\n",
        "            x = self.GRCL[i](wgf_u, self.wgr_x(x), wf_u, self.wr_x(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GRCL_unit(nn.Module):\n",
        "\n",
        "    def __init__(self, output_channel):\n",
        "        super(GRCL_unit, self).__init__()\n",
        "        self.BN_gfu = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_grx = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_fu = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_rx = nn.BatchNorm2d(output_channel)\n",
        "        self.BN_Gx = nn.BatchNorm2d(output_channel)\n",
        "\n",
        "    def forward(self, wgf_u, wgr_x, wf_u, wr_x):\n",
        "        G_first_term = self.BN_gfu(wgf_u)\n",
        "        G_second_term = self.BN_grx(wgr_x)\n",
        "        G = F.sigmoid(G_first_term + G_second_term)\n",
        "\n",
        "        x_first_term = self.BN_fu(wf_u)\n",
        "        x_second_term = self.BN_Gx(self.BN_rx(wr_x) * G)\n",
        "        x = F.relu(x_first_term + x_second_term)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = self._conv3x3(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = self._conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def _conv3x3(self, in_planes, out_planes, stride=1):\n",
        "        \"3x3 convolution with padding\"\n",
        "        return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                         padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, block, layers):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.output_channel_block = [int(output_channel / 4), int(output_channel / 2), output_channel, output_channel]\n",
        "\n",
        "        self.inplanes = int(output_channel / 8)\n",
        "        self.conv0_1 = nn.Conv2d(input_channel, int(output_channel / 16),\n",
        "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn0_1 = nn.BatchNorm2d(int(output_channel / 16))\n",
        "        self.conv0_2 = nn.Conv2d(int(output_channel / 16), self.inplanes,\n",
        "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn0_2 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.layer1 = self._make_layer(block, self.output_channel_block[0], layers[0])\n",
        "        self.conv1 = nn.Conv2d(self.output_channel_block[0], self.output_channel_block[\n",
        "                               0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.output_channel_block[0])\n",
        "\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.layer2 = self._make_layer(block, self.output_channel_block[1], layers[1], stride=1)\n",
        "        self.conv2 = nn.Conv2d(self.output_channel_block[1], self.output_channel_block[\n",
        "                               1], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(self.output_channel_block[1])\n",
        "\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
        "        self.layer3 = self._make_layer(block, self.output_channel_block[2], layers[2], stride=1)\n",
        "        self.conv3 = nn.Conv2d(self.output_channel_block[2], self.output_channel_block[\n",
        "                               2], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.output_channel_block[2])\n",
        "\n",
        "        self.layer4 = self._make_layer(block, self.output_channel_block[3], layers[3], stride=1)\n",
        "        self.conv4_1 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
        "                                 3], kernel_size=2, stride=(2, 1), padding=(0, 1), bias=False)\n",
        "        self.bn4_1 = nn.BatchNorm2d(self.output_channel_block[3])\n",
        "        self.conv4_2 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
        "                                 3], kernel_size=2, stride=1, padding=0, bias=False)\n",
        "        self.bn4_2 = nn.BatchNorm2d(self.output_channel_block[3])\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0_1(x)\n",
        "        x = self.bn0_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv0_2(x)\n",
        "        x = self.bn0_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.bn4_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.bn4_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MCkLwTesJPnF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(Model, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.stages = {'Trans': opt.Transformation, 'Feat': opt.FeatureExtraction,\n",
        "                       'Seq': opt.SequenceModeling, 'Pred': opt.Prediction}\n",
        "\n",
        "        \"\"\" Transformation \"\"\"\n",
        "        if opt.Transformation == 'TPS':\n",
        "            self.Transformation = TPS_SpatialTransformerNetwork(\n",
        "                F=opt.num_fiducial, I_size=(opt.imgH, opt.imgW), I_r_size=(opt.imgH, opt.imgW), I_channel_num=opt.input_channel)\n",
        "        else:\n",
        "            print('No Transformation module specified')\n",
        "\n",
        "        \"\"\" FeatureExtraction \"\"\"\n",
        "        if opt.FeatureExtraction == 'VGG':\n",
        "            self.FeatureExtraction = VGG_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        elif opt.FeatureExtraction == 'RCNN':\n",
        "            self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        elif opt.FeatureExtraction == 'ResNet':\n",
        "            self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        else:\n",
        "            raise Exception('No FeatureExtraction module specified')\n",
        "        self.FeatureExtraction_output = opt.output_channel  # int(imgH/16-1) * 512\n",
        "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
        "\n",
        "        \"\"\" Sequence modeling\"\"\"\n",
        "        if opt.SequenceModeling == 'BiLSTM':\n",
        "            self.SequenceModeling = nn.Sequential(\n",
        "                BidirectionalLSTM(self.FeatureExtraction_output, opt.hidden_size, opt.hidden_size),\n",
        "                BidirectionalLSTM(opt.hidden_size, opt.hidden_size, opt.hidden_size))\n",
        "            self.SequenceModeling_output = opt.hidden_size\n",
        "        else:\n",
        "            print('No SequenceModeling module specified')\n",
        "            self.SequenceModeling_output = self.FeatureExtraction_output\n",
        "\n",
        "        \"\"\" Prediction \"\"\"\n",
        "        if opt.Prediction == 'CTC':\n",
        "            self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)\n",
        "        elif opt.Prediction == 'Attn':\n",
        "            self.Prediction = Attention(self.SequenceModeling_output, opt.hidden_size, opt.num_class)\n",
        "        else:\n",
        "            raise Exception('Prediction is neither CTC or Attn')\n",
        "\n",
        "    def forward(self, input, text, is_train=True):\n",
        "        \"\"\" Transformation stage \"\"\"\n",
        "        if not self.stages['Trans'] == \"None\":\n",
        "            input = self.Transformation(input)\n",
        "\n",
        "        \"\"\" Feature extraction stage \"\"\"\n",
        "        visual_feature = self.FeatureExtraction(input)\n",
        "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
        "        visual_feature = visual_feature.squeeze(3)\n",
        "\n",
        "        \"\"\" Sequence modeling stage \"\"\"\n",
        "        if self.stages['Seq'] == 'BiLSTM':\n",
        "            contextual_feature = self.SequenceModeling(visual_feature)\n",
        "        else:\n",
        "            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM\n",
        "\n",
        "        \"\"\" Prediction stage \"\"\"\n",
        "        if self.stages['Pred'] == 'CTC':\n",
        "            prediction = self.Prediction(contextual_feature.contiguous())\n",
        "        else:\n",
        "            prediction = self.Prediction(contextual_feature.contiguous(), text, is_train, batch_max_length=self.opt.batch_max_length)\n",
        "\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "O-yj4Y2UMc3r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "##### https://github.com/githubharald/CTCDecoder/blob/master/src/BeamSearch.py\n",
        "class BeamEntry:\n",
        "    \"information about one single beam at specific time-step\"\n",
        "    def __init__(self):\n",
        "        self.prTotal = 0 # blank and non-blank\n",
        "        self.prNonBlank = 0 # non-blank\n",
        "        self.prBlank = 0 # blank\n",
        "        self.prText = 1 # LM score\n",
        "        self.lmApplied = False # flag if LM was already applied to this beam\n",
        "        self.labeling = () # beam-labeling\n",
        "\n",
        "class BeamState:\n",
        "    \"information about the beams at specific time-step\"\n",
        "    def __init__(self):\n",
        "        self.entries = {}\n",
        "\n",
        "    def norm(self):\n",
        "        \"length-normalise LM score\"\n",
        "        for (k, _) in self.entries.items():\n",
        "            labelingLen = len(self.entries[k].labeling)\n",
        "            self.entries[k].prText = self.entries[k].prText ** (1.0 / (labelingLen if labelingLen else 1.0))\n",
        "\n",
        "    def sort(self):\n",
        "        \"return beam-labelings, sorted by probability\"\n",
        "        beams = [v for (_, v) in self.entries.items()]\n",
        "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n",
        "        return [x.labeling for x in sortedBeams]\n",
        "\n",
        "    def wordsearch(self, classes, ignore_idx, beamWidth, dict_list):\n",
        "        beams = [v for (_, v) in self.entries.items()]\n",
        "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)[:beamWidth]\n",
        "\n",
        "        for j, candidate in enumerate(sortedBeams):\n",
        "            idx_list = candidate.labeling\n",
        "            text = ''\n",
        "            for i,l in enumerate(idx_list):\n",
        "                if l not in ignore_idx and (not (i > 0 and idx_list[i - 1] == idx_list[i])):  # removing repeated characters and blank.\n",
        "                    text += classes[l]\n",
        "\n",
        "            if j == 0: best_text = text\n",
        "            if text in dict_list:\n",
        "                print('found text: ', text)\n",
        "                best_text = text\n",
        "                break\n",
        "            else:\n",
        "                print('not in dict: ', text)\n",
        "        return best_text\n",
        "\n",
        "def applyLM(parentBeam, childBeam, classes, lm):\n",
        "    \"calculate LM score of child beam by taking score from parent beam and bigram probability of last two chars\"\n",
        "    if lm and not childBeam.lmApplied:\n",
        "        c1 = classes[parentBeam.labeling[-1] if parentBeam.labeling else classes.index(' ')] # first char\n",
        "        c2 = classes[childBeam.labeling[-1]] # second char\n",
        "        lmFactor = 0.01 # influence of language model\n",
        "        bigramProb = lm.getCharBigram(c1, c2) ** lmFactor # probability of seeing first and second char next to each other\n",
        "        childBeam.prText = parentBeam.prText * bigramProb # probability of char sequence\n",
        "        childBeam.lmApplied = True # only apply LM once per beam entry\n",
        "\n",
        "def addBeam(beamState, labeling):\n",
        "    \"add beam if it does not yet exist\"\n",
        "    if labeling not in beamState.entries:\n",
        "        beamState.entries[labeling] = BeamEntry()\n",
        "\n",
        "def ctcBeamSearch(mat, classes, ignore_idx, lm, beamWidth=25, dict_list = []):\n",
        "    \"beam search as described by the paper of Hwang et al. and the paper of Graves et al.\"\n",
        "\n",
        "    #blankIdx = len(classes)\n",
        "    blankIdx = 0\n",
        "    maxT, maxC = mat.shape\n",
        "\n",
        "    # initialise beam state\n",
        "    last = BeamState()\n",
        "    labeling = ()\n",
        "    last.entries[labeling] = BeamEntry()\n",
        "    last.entries[labeling].prBlank = 1\n",
        "    last.entries[labeling].prTotal = 1\n",
        "\n",
        "    # go over all time-steps\n",
        "    for t in range(maxT):\n",
        "        curr = BeamState()\n",
        "\n",
        "        # get beam-labelings of best beams\n",
        "        bestLabelings = last.sort()[0:beamWidth]\n",
        "\n",
        "        # go over best beams\n",
        "        for labeling in bestLabelings:\n",
        "\n",
        "            # probability of paths ending with a non-blank\n",
        "            prNonBlank = 0\n",
        "            # in case of non-empty beam\n",
        "            if labeling:\n",
        "                # probability of paths with repeated last char at the end\n",
        "                prNonBlank = last.entries[labeling].prNonBlank * mat[t, labeling[-1]]\n",
        "\n",
        "            # probability of paths ending with a blank\n",
        "            prBlank = (last.entries[labeling].prTotal) * mat[t, blankIdx]\n",
        "\n",
        "            # add beam at current time-step if needed\n",
        "            addBeam(curr, labeling)\n",
        "\n",
        "            # fill in data\n",
        "            curr.entries[labeling].labeling = labeling\n",
        "            curr.entries[labeling].prNonBlank += prNonBlank\n",
        "            curr.entries[labeling].prBlank += prBlank\n",
        "            curr.entries[labeling].prTotal += prBlank + prNonBlank\n",
        "            curr.entries[labeling].prText = last.entries[labeling].prText # beam-labeling not changed, therefore also LM score unchanged from\n",
        "            curr.entries[labeling].lmApplied = True # LM already applied at previous time-step for this beam-labeling\n",
        "\n",
        "            # extend current beam-labeling\n",
        "            for c in range(maxC - 1):\n",
        "                # add new char to current beam-labeling\n",
        "                newLabeling = labeling + (c,)\n",
        "\n",
        "                # if new labeling contains duplicate char at the end, only consider paths ending with a blank\n",
        "                if labeling and labeling[-1] == c:\n",
        "                    prNonBlank = mat[t, c] * last.entries[labeling].prBlank\n",
        "                else:\n",
        "                    prNonBlank = mat[t, c] * last.entries[labeling].prTotal\n",
        "\n",
        "                # add beam at current time-step if needed\n",
        "                addBeam(curr, newLabeling)\n",
        "\n",
        "                # fill in data\n",
        "                curr.entries[newLabeling].labeling = newLabeling\n",
        "                curr.entries[newLabeling].prNonBlank += prNonBlank\n",
        "                curr.entries[newLabeling].prTotal += prNonBlank\n",
        "\n",
        "                # apply LM\n",
        "                #applyLM(curr.entries[labeling], curr.entries[newLabeling], classes, lm)\n",
        "\n",
        "        # set new beam state\n",
        "        last = curr\n",
        "\n",
        "    # normalise LM scores according to beam-labeling-length\n",
        "    last.norm()\n",
        "\n",
        "    # sort by probability\n",
        "    #bestLabeling = last.sort()[0] # get most probable labeling\n",
        "\n",
        "    # map labels to chars\n",
        "    #res = ''\n",
        "    #for idx,l in enumerate(bestLabeling):\n",
        "    #    if l not in ignore_idx and (not (idx > 0 and bestLabeling[idx - 1] == bestLabeling[idx])):  # removing repeated characters and blank.\n",
        "    #        res += classes[l]\n",
        "\n",
        "    if dict_list == []:\n",
        "        bestLabeling = last.sort()[0] # get most probable labeling\n",
        "        res = ''\n",
        "        for i,l in enumerate(bestLabeling):\n",
        "            if l not in ignore_idx and (not (i > 0 and bestLabeling[i - 1] == bestLabeling[i])):  # removing repeated characters and blank.\n",
        "                res += classes[l]\n",
        "    else:\n",
        "        res = last.wordsearch(classes, ignore_idx, beamWidth, dict_list)\n",
        "\n",
        "    return res\n",
        "#####\n",
        "\n",
        "def consecutive(data, mode ='first', stepsize=1):\n",
        "    group = np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
        "    group = [item for item in group if len(item)>0]\n",
        "\n",
        "    if mode == 'first': result = [l[0] for l in group]\n",
        "    elif mode == 'last': result = [l[-1] for l in group]\n",
        "    return result\n",
        "\n",
        "def word_segmentation(mat, separator_idx =  {'th': [1,2],'en': [3,4]}, separator_idx_list = [1,2,3,4]):\n",
        "    result = []\n",
        "    sep_list = []\n",
        "    start_idx = 0\n",
        "    for sep_idx in separator_idx_list:\n",
        "        if sep_idx % 2 == 0: mode ='first'\n",
        "        else: mode ='last'\n",
        "        a = consecutive( np.argwhere(mat == sep_idx).flatten(), mode)\n",
        "        new_sep = [ [item, sep_idx] for item in a]\n",
        "        sep_list += new_sep\n",
        "    sep_list = sorted(sep_list, key=lambda x: x[0])\n",
        "\n",
        "    for sep in sep_list:\n",
        "        for lang in separator_idx.keys():\n",
        "            if sep[1] == separator_idx[lang][0]: # start lang\n",
        "                sep_lang = lang\n",
        "                sep_start_idx = sep[0]\n",
        "            elif sep[1] == separator_idx[lang][1]: # end lang\n",
        "                if sep_lang == lang: # check if last entry if the same start lang\n",
        "                    new_sep_pair = [lang, [sep_start_idx+1, sep[0]-1]]\n",
        "                    if sep_start_idx > start_idx:\n",
        "                        result.append( ['', [start_idx, sep_start_idx-1] ] )\n",
        "                    start_idx = sep[0]+1\n",
        "                    result.append(new_sep_pair)\n",
        "                else: # reset\n",
        "                    sep_lang = ''\n",
        "\n",
        "    if start_idx <= len(mat)-1:\n",
        "        result.append( ['', [start_idx, len(mat)-1] ] )\n",
        "    return result\n",
        "\n",
        "class CTCLabelConverter(object):\n",
        "    \"\"\" Convert between text-label and text-index \"\"\"\n",
        "\n",
        "    #def __init__(self, character, separator = []):\n",
        "    def __init__(self, character, separator_list = {}, dict_pathlist = {}):\n",
        "        # character (str): set of the possible characters.\n",
        "        dict_character = list(character)\n",
        "\n",
        "        #special_character = ['\\xa2', '\\xa3', '\\xa4','\\xa5']\n",
        "        #self.separator_char = special_character[:len(separator)]\n",
        "\n",
        "        self.dict = {}\n",
        "        #for i, char in enumerate(self.separator_char + dict_character):\n",
        "        for i, char in enumerate(dict_character):\n",
        "            # NOTE: 0 is reserved for 'blank' token required by CTCLoss\n",
        "            self.dict[char] = i + 1\n",
        "\n",
        "        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
        "        #self.character = ['[blank]']+ self.separator_char + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
        "        self.separator_list = separator_list\n",
        "\n",
        "        separator_char = []\n",
        "        for lang, sep in separator_list.items():\n",
        "            separator_char += sep\n",
        "\n",
        "        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]\n",
        "\n",
        "        dict_list = {}\n",
        "        for lang, dict_path in dict_pathlist.items():\n",
        "            with open(dict_path, \"rb\") as input_file:\n",
        "                word_count = pickle.load(input_file)\n",
        "            dict_list[lang] = word_count\n",
        "        self.dict_list = dict_list\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\"convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "\n",
        "        output:\n",
        "            text: concatenated text index for CTCLoss.\n",
        "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
        "            length: length of each text. [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) for s in text]\n",
        "        text = ''.join(text)\n",
        "        text = [self.dict[char] for char in text]\n",
        "\n",
        "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
        "\n",
        "    def decode_greedy(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        index = 0\n",
        "        for l in length:\n",
        "            t = text_index[index:index + l]\n",
        "\n",
        "            char_list = []\n",
        "            for i in range(l):\n",
        "                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
        "                #if (t[i] != 0) and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
        "                    char_list.append(self.character[t[i]])\n",
        "            text = ''.join(char_list)\n",
        "\n",
        "            texts.append(text)\n",
        "            index += l\n",
        "        return texts\n",
        "\n",
        "    def decode_beamsearch(self, mat, beamWidth=5):\n",
        "        texts = []\n",
        "\n",
        "        for i in range(mat.shape[0]):\n",
        "            t = ctcBeamSearch(mat[i], self.character, self.ignore_idx, None, beamWidth=beamWidth)\n",
        "            texts.append(t)\n",
        "        return texts\n",
        "\n",
        "    def decode_wordbeamsearch(self, mat, beamWidth=5):\n",
        "        texts = []\n",
        "        argmax = np.argmax(mat, axis = 2)\n",
        "        for i in range(mat.shape[0]):\n",
        "            words = word_segmentation(argmax[i])\n",
        "            string = ''\n",
        "            for word in words:\n",
        "                matrix = mat[i, word[1][0]:word[1][1]+1,:]\n",
        "                if word[0] == '': dict_list = []\n",
        "                else: dict_list = self.dict_list[word[0]]\n",
        "                t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None, beamWidth=beamWidth, dict_list=dict_list)\n",
        "                string += t\n",
        "            texts.append(string)\n",
        "        return texts\n",
        "\n",
        "class AttnLabelConverter(object):\n",
        "    \"\"\" Convert between text-label and text-index \"\"\"\n",
        "\n",
        "    def __init__(self, character):\n",
        "        # character (str): set of the possible characters.\n",
        "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
        "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
        "        list_character = list(character)\n",
        "        self.character = list_token + list_character\n",
        "\n",
        "        self.dict = {}\n",
        "        for i, char in enumerate(self.character):\n",
        "            # print(i, char)\n",
        "            self.dict[char] = i\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\" convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "            batch_max_length: max length of text label in the batch. 25 by default\n",
        "\n",
        "        output:\n",
        "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
        "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
        "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
        "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
        "        batch_max_length += 1\n",
        "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
        "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
        "        for i, t in enumerate(text):\n",
        "            text = list(t)\n",
        "            text.append('[s]')\n",
        "            text = [self.dict[char] for char in text]\n",
        "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
        "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
        "\n",
        "    def decode(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        for index, l in enumerate(length):\n",
        "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
        "            texts.append(text)\n",
        "        return texts\n",
        "\n",
        "\n",
        "class Averager(object):\n",
        "    \"\"\"Compute average for torch.Tensor, used for loss average.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def add(self, v):\n",
        "        count = v.data.numel()\n",
        "        v = v.data.sum()\n",
        "        self.n_count += count\n",
        "        self.sum += v\n",
        "\n",
        "    def reset(self):\n",
        "        self.n_count = 0\n",
        "        self.sum = 0\n",
        "\n",
        "    def val(self):\n",
        "        res = 0\n",
        "        if self.n_count != 0:\n",
        "            res = self.sum / float(self.n_count)\n",
        "        return res\n",
        "def _accumulate(iterable, fn=lambda x, y: x + y):\n",
        "    \"Return running totals\"\n",
        "    # _accumulate([1,2,3,4,5]) --> 1 3 6 10 15\n",
        "    # _accumulate([1,2,3,4,5], operator.mul) --> 1 2 6 24 120\n",
        "    it = iter(iterable)\n",
        "    try:\n",
        "        total = next(it)\n",
        "    except StopIteration:\n",
        "        return\n",
        "    yield total\n",
        "    for element in it:\n",
        "        total = fn(total, element)\n",
        "        yield total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TeGsvCPtJLKy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import six\n",
        "import math\n",
        "import torch\n",
        "import pandas  as pd\n",
        "\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def contrast_grey(img):\n",
        "    high = np.percentile(img, 90)\n",
        "    low  = np.percentile(img, 10)\n",
        "    return (high-low)/(high+low), high, low\n",
        "\n",
        "def adjust_contrast_grey(img, target = 0.4):\n",
        "    contrast, high, low = contrast_grey(img)\n",
        "    if contrast < target:\n",
        "        img = img.astype(int)\n",
        "        ratio = 200./(high-low)\n",
        "        img = (img - low + 25)*ratio\n",
        "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "\n",
        "class Batch_Balanced_Dataset(object):\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"\n",
        "        Modulate the data ratio in the batch.\n",
        "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
        "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
        "        \"\"\"\n",
        "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
        "        dashed_line = '-' * 80\n",
        "        print(dashed_line)\n",
        "        log.write(dashed_line + '\\n')\n",
        "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
        "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
        "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
        "\n",
        "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
        "        self.data_loader_list = []\n",
        "        self.dataloader_iter_list = []\n",
        "        batch_size_list = []\n",
        "        Total_batch_size = 0\n",
        "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
        "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
        "            print(dashed_line)\n",
        "            log.write(dashed_line + '\\n')\n",
        "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
        "            total_number_dataset = len(_dataset)\n",
        "            log.write(_dataset_log)\n",
        "\n",
        "            \"\"\"\n",
        "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
        "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
        "            See 4.2 section in our paper.\n",
        "            \"\"\"\n",
        "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
        "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
        "            indices = range(total_number_dataset)\n",
        "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
        "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
        "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
        "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
        "            print(selected_d_log)\n",
        "            log.write(selected_d_log + '\\n')\n",
        "            batch_size_list.append(str(_batch_size))\n",
        "            Total_batch_size += _batch_size\n",
        "\n",
        "            _data_loader = torch.utils.data.DataLoader(\n",
        "                _dataset, batch_size=_batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
        "                collate_fn=_AlignCollate, pin_memory=True)\n",
        "            self.data_loader_list.append(_data_loader)\n",
        "            self.dataloader_iter_list.append(iter(_data_loader))\n",
        "\n",
        "        Total_batch_size_log = f'{dashed_line}\\n'\n",
        "        batch_size_sum = '+'.join(batch_size_list)\n",
        "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
        "        Total_batch_size_log += f'{dashed_line}'\n",
        "        opt.batch_size = Total_batch_size\n",
        "\n",
        "        print(Total_batch_size_log)\n",
        "        log.write(Total_batch_size_log + '\\n')\n",
        "        log.close()\n",
        "\n",
        "    def get_batch(self):\n",
        "        balanced_batch_images = []\n",
        "        balanced_batch_texts = []\n",
        "\n",
        "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
        "            try:\n",
        "                image, text = data_loader_iter.next()\n",
        "                balanced_batch_images.append(image)\n",
        "                balanced_batch_texts += text\n",
        "            except StopIteration:\n",
        "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
        "                image, text = self.dataloader_iter_list[i].next()\n",
        "                balanced_batch_images.append(image)\n",
        "                balanced_batch_texts += text\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
        "\n",
        "        return balanced_batch_images, balanced_batch_texts\n",
        "\n",
        "\n",
        "def hierarchical_dataset(root, opt, select_data='/'):\n",
        "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
        "    dataset_list = []\n",
        "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
        "    print(dataset_log)\n",
        "    dataset_log += '\\n'\n",
        "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
        "        if not dirnames:\n",
        "            select_flag = False\n",
        "            for selected_d in select_data:\n",
        "                if selected_d in dirpath:\n",
        "                    select_flag = True\n",
        "                    break\n",
        "\n",
        "            if select_flag:\n",
        "                dataset = OCRDataset(dirpath, opt)\n",
        "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
        "                print(sub_dataset_log)\n",
        "                dataset_log += f'{sub_dataset_log}\\n'\n",
        "                dataset_list.append(dataset)\n",
        "\n",
        "    concatenated_dataset = ConcatDataset(dataset_list)\n",
        "\n",
        "    return concatenated_dataset, dataset_log\n",
        "\n",
        "class OCRDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, opt):\n",
        "\n",
        "        self.root = root\n",
        "        self.opt = opt\n",
        "        print(root)\n",
        "        self.df = pd.read_csv(os.path.join(root,'label.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'text'], keep_default_na=False)\n",
        "        self.nSamples = len(self.df)\n",
        "\n",
        "        if self.opt.data_filtering_off:\n",
        "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
        "        else:\n",
        "            self.filtered_index_list = []\n",
        "            for index in range(self.nSamples):\n",
        "                label = self.df.at[index,'text']\n",
        "                try:\n",
        "                    if len(label) > self.opt.batch_max_length:\n",
        "                        continue\n",
        "                except:\n",
        "                    print(label)\n",
        "                out_of_char = f'[^{self.opt.character}]'\n",
        "                if re.search(out_of_char, label.lower()):\n",
        "                    continue\n",
        "                self.filtered_index_list.append(index)\n",
        "            self.nSamples = len(self.filtered_index_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.filtered_index_list[index]\n",
        "        img_fname = self.df.at[index,'filename']\n",
        "        img_fpath = os.path.join(self.root, img_fname)\n",
        "        label = self.df.at[index,'text']\n",
        "\n",
        "        if self.opt.rgb:\n",
        "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
        "        else:\n",
        "            img = Image.open(img_fpath).convert('L')\n",
        "\n",
        "        if not self.opt.sensitive:\n",
        "            label = label.lower()\n",
        "\n",
        "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
        "        out_of_char = f'[^{self.opt.character}]'\n",
        "        label = re.sub(out_of_char, '', label)\n",
        "\n",
        "        return (img, label)\n",
        "\n",
        "class ResizeNormalize(object):\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = img.resize(self.size, self.interpolation)\n",
        "        img = self.toTensor(img)\n",
        "        img.sub_(0.5).div_(0.5)\n",
        "        return img\n",
        "\n",
        "\n",
        "class NormalizePAD(object):\n",
        "\n",
        "    def __init__(self, max_size, PAD_type='right'):\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "        self.max_size = max_size\n",
        "        self.max_width_half = math.floor(max_size[2] / 2)\n",
        "        self.PAD_type = PAD_type\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = self.toTensor(img)\n",
        "        img.sub_(0.5).div_(0.5)\n",
        "        c, h, w = img.size()\n",
        "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
        "        Pad_img[:, :, :w] = img  # right pad\n",
        "        if self.max_size[2] != w:  # add border Pad\n",
        "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
        "\n",
        "        return Pad_img\n",
        "\n",
        "\n",
        "class AlignCollate(object):\n",
        "\n",
        "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
        "        self.imgH = imgH\n",
        "        self.imgW = imgW\n",
        "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
        "        self.contrast_adjust = contrast_adjust\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = filter(lambda x: x is not None, batch)\n",
        "        images, labels = zip(*batch)\n",
        "\n",
        "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
        "            resized_max_w = self.imgW\n",
        "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
        "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
        "\n",
        "            resized_images = []\n",
        "            for image in images:\n",
        "                w, h = image.size\n",
        "\n",
        "                #### augmentation here - change contrast\n",
        "                if self.contrast_adjust > 0:\n",
        "                    image = np.array(image.convert(\"L\"))\n",
        "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
        "                    image = Image.fromarray(image, 'L')\n",
        "\n",
        "                ratio = w / float(h)\n",
        "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
        "                    resized_w = self.imgW\n",
        "                else:\n",
        "                    resized_w = math.ceil(self.imgH * ratio)\n",
        "\n",
        "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
        "                resized_images.append(transform(resized_image))\n",
        "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
        "\n",
        "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
        "\n",
        "        else:\n",
        "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
        "            image_tensors = [transform(image) for image in images]\n",
        "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
        "\n",
        "        return image_tensors, labels\n",
        "\n",
        "\n",
        "def tensor2im(image_tensor, imtype=np.uint8):\n",
        "    image_numpy = image_tensor.cpu().float().numpy()\n",
        "    if image_numpy.shape[0] == 1:\n",
        "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "def save_image(image_numpy, image_path):\n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "    image_pil.save(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SXGlpOxDq2JM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JXh7fKuOMw2i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import six\n",
        "import math\n",
        "import torch\n",
        "import pandas  as pd\n",
        "\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def contrast_grey(img):\n",
        "    high = np.percentile(img, 90)\n",
        "    low  = np.percentile(img, 10)\n",
        "    return (high-low)/(high+low), high, low\n",
        "\n",
        "def adjust_contrast_grey(img, target = 0.4):\n",
        "    contrast, high, low = contrast_grey(img)\n",
        "    if contrast < target:\n",
        "        img = img.astype(int)\n",
        "        ratio = 200./(high-low)\n",
        "        img = (img - low + 25)*ratio\n",
        "        img = np.maximum(np.full(img.shape, 0) ,np.minimum(np.full(img.shape, 255), img)).astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "\n",
        "class Batch_Balanced_Dataset(object):\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"\n",
        "        Modulate the data ratio in the batch.\n",
        "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
        "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
        "        \"\"\"\n",
        "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
        "        dashed_line = '-' * 80\n",
        "        print(dashed_line)\n",
        "        log.write(dashed_line + '\\n')\n",
        "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
        "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
        "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
        "\n",
        "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
        "        self.data_loader_list = []\n",
        "        self.dataloader_iter_list = []\n",
        "        batch_size_list = []\n",
        "        Total_batch_size = 0\n",
        "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
        "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
        "            print(dashed_line)\n",
        "            log.write(dashed_line + '\\n')\n",
        "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
        "            total_number_dataset = len(_dataset)\n",
        "            log.write(_dataset_log)\n",
        "\n",
        "            \"\"\"\n",
        "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
        "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
        "            See 4.2 section in our paper.\n",
        "            \"\"\"\n",
        "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
        "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
        "            indices = range(total_number_dataset)\n",
        "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
        "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
        "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
        "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
        "            print(selected_d_log)\n",
        "            log.write(selected_d_log + '\\n')\n",
        "            batch_size_list.append(str(_batch_size))\n",
        "            Total_batch_size += _batch_size\n",
        "\n",
        "            _data_loader = torch.utils.data.DataLoader(\n",
        "                _dataset, batch_size=_batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
        "                collate_fn=_AlignCollate, pin_memory=True)\n",
        "            self.data_loader_list.append(_data_loader)\n",
        "            self.dataloader_iter_list.append(iter(_data_loader))\n",
        "\n",
        "        Total_batch_size_log = f'{dashed_line}\\n'\n",
        "        batch_size_sum = '+'.join(batch_size_list)\n",
        "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
        "        Total_batch_size_log += f'{dashed_line}'\n",
        "        opt.batch_size = Total_batch_size\n",
        "\n",
        "        print(Total_batch_size_log)\n",
        "        log.write(Total_batch_size_log + '\\n')\n",
        "        log.close()\n",
        "\n",
        "    def get_batch(self):\n",
        "        balanced_batch_images = []\n",
        "        balanced_batch_texts = []\n",
        "\n",
        "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
        "            try:\n",
        "                image, text = next(data_loader_iter)\n",
        "                balanced_batch_images.append(image)\n",
        "                balanced_batch_texts += text\n",
        "            except StopIteration:\n",
        "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
        "                image, text = next(self.dataloader_iter_list[i])\n",
        "                balanced_batch_images.append(image)\n",
        "                balanced_batch_texts += text\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
        "\n",
        "        return balanced_batch_images, balanced_batch_texts\n",
        "\n",
        "\n",
        "def hierarchical_dataset(root, opt, select_data='/'):\n",
        "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
        "    dataset_list = []\n",
        "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
        "    print(dataset_log)\n",
        "    dataset_log += '\\n'\n",
        "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
        "        if not dirnames:\n",
        "            select_flag = False\n",
        "            for selected_d in select_data:\n",
        "                if selected_d in dirpath:\n",
        "                    select_flag = True\n",
        "                    break\n",
        "\n",
        "            if select_flag:\n",
        "                dataset = OCRDataset(dirpath, opt)\n",
        "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
        "                print(sub_dataset_log)\n",
        "                dataset_log += f'{sub_dataset_log}\\n'\n",
        "                dataset_list.append(dataset)\n",
        "\n",
        "    concatenated_dataset = ConcatDataset(dataset_list)\n",
        "\n",
        "    return concatenated_dataset, dataset_log\n",
        "\n",
        "class OCRDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, opt):\n",
        "\n",
        "        self.root = root\n",
        "        self.opt = opt\n",
        "        print(root)\n",
        "        self.df = pd.read_csv(os.path.join(root,'label.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'text'], keep_default_na=False)\n",
        "        self.nSamples = len(self.df)\n",
        "\n",
        "        if self.opt.data_filtering_off:\n",
        "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
        "        else:\n",
        "            self.filtered_index_list = []\n",
        "            for index in range(self.nSamples):\n",
        "                label = self.df.at[index,'text']\n",
        "                try:\n",
        "                    if len(label) > self.opt.batch_max_length:\n",
        "                        continue\n",
        "                except:\n",
        "                    print(label)\n",
        "                out_of_char = f'[^{self.opt.character}]'\n",
        "                if re.search(out_of_char, label.lower()):\n",
        "                    continue\n",
        "                self.filtered_index_list.append(index)\n",
        "            self.nSamples = len(self.filtered_index_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.filtered_index_list[index]\n",
        "        img_fname = self.df.at[index,'filename']\n",
        "        img_fpath = os.path.join(self.root, img_fname)\n",
        "        label = self.df.at[index,'text']\n",
        "\n",
        "        if self.opt.rgb:\n",
        "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
        "        else:\n",
        "            img = Image.open(img_fpath).convert('L')\n",
        "\n",
        "        if not self.opt.sensitive:\n",
        "            label = label.lower()\n",
        "\n",
        "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
        "        out_of_char = f'[^{self.opt.character}]'\n",
        "        label = re.sub(out_of_char, '', label)\n",
        "\n",
        "        return (img, label)\n",
        "\n",
        "class ResizeNormalize(object):\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = img.resize(self.size, self.interpolation)\n",
        "        img = self.toTensor(img)\n",
        "        img.sub_(0.5).div_(0.5)\n",
        "        return img\n",
        "\n",
        "\n",
        "class NormalizePAD(object):\n",
        "\n",
        "    def __init__(self, max_size, PAD_type='right'):\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "        self.max_size = max_size\n",
        "        self.max_width_half = math.floor(max_size[2] / 2)\n",
        "        self.PAD_type = PAD_type\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = self.toTensor(img)\n",
        "        img.sub_(0.5).div_(0.5)\n",
        "        c, h, w = img.size()\n",
        "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
        "        Pad_img[:, :, :w] = img  # right pad\n",
        "        if self.max_size[2] != w:  # add border Pad\n",
        "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
        "\n",
        "        return Pad_img\n",
        "\n",
        "\n",
        "class AlignCollate(object):\n",
        "\n",
        "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
        "        self.imgH = imgH\n",
        "        self.imgW = imgW\n",
        "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
        "        self.contrast_adjust = contrast_adjust\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = filter(lambda x: x is not None, batch)\n",
        "        images, labels = zip(*batch)\n",
        "\n",
        "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
        "            resized_max_w = self.imgW\n",
        "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
        "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
        "\n",
        "            resized_images = []\n",
        "            for image in images:\n",
        "                w, h = image.size\n",
        "\n",
        "                #### augmentation here - change contrast\n",
        "                if self.contrast_adjust > 0:\n",
        "                    image = np.array(image.convert(\"L\"))\n",
        "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
        "                    image = Image.fromarray(image, 'L')\n",
        "\n",
        "                ratio = w / float(h)\n",
        "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
        "                    resized_w = self.imgW\n",
        "                else:\n",
        "                    resized_w = math.ceil(self.imgH * ratio)\n",
        "\n",
        "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
        "                resized_images.append(transform(resized_image))\n",
        "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
        "\n",
        "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
        "\n",
        "        else:\n",
        "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
        "            image_tensors = [transform(image) for image in images]\n",
        "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
        "\n",
        "        return image_tensors, labels\n",
        "\n",
        "\n",
        "def tensor2im(image_tensor, imtype=np.uint8):\n",
        "    image_numpy = image_tensor.cpu().float().numpy()\n",
        "    if image_numpy.shape[0] == 1:\n",
        "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "def save_image(image_numpy, image_path):\n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "    image_pil.save(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_rMQZg_KTCzI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "##### https://github.com/githubharald/CTCDecoder/blob/master/src/BeamSearch.py\n",
        "class BeamEntry:\n",
        "    \"information about one single beam at specific time-step\"\n",
        "    def __init__(self):\n",
        "        self.prTotal = 0 # blank and non-blank\n",
        "        self.prNonBlank = 0 # non-blank\n",
        "        self.prBlank = 0 # blank\n",
        "        self.prText = 1 # LM score\n",
        "        self.lmApplied = False # flag if LM was already applied to this beam\n",
        "        self.labeling = () # beam-labeling\n",
        "\n",
        "class BeamState:\n",
        "    \"information about the beams at specific time-step\"\n",
        "    def __init__(self):\n",
        "        self.entries = {}\n",
        "\n",
        "    def norm(self):\n",
        "        \"length-normalise LM score\"\n",
        "        for (k, _) in self.entries.items():\n",
        "            labelingLen = len(self.entries[k].labeling)\n",
        "            self.entries[k].prText = self.entries[k].prText ** (1.0 / (labelingLen if labelingLen else 1.0))\n",
        "\n",
        "    def sort(self):\n",
        "        \"return beam-labelings, sorted by probability\"\n",
        "        beams = [v for (_, v) in self.entries.items()]\n",
        "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n",
        "        return [x.labeling for x in sortedBeams]\n",
        "\n",
        "    def wordsearch(self, classes, ignore_idx, beamWidth, dict_list):\n",
        "        beams = [v for (_, v) in self.entries.items()]\n",
        "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)[:beamWidth]\n",
        "\n",
        "        for j, candidate in enumerate(sortedBeams):\n",
        "            idx_list = candidate.labeling\n",
        "            text = ''\n",
        "            for i,l in enumerate(idx_list):\n",
        "                if l not in ignore_idx and (not (i > 0 and idx_list[i - 1] == idx_list[i])):  # removing repeated characters and blank.\n",
        "                    text += classes[l]\n",
        "\n",
        "            if j == 0: best_text = text\n",
        "            if text in dict_list:\n",
        "                print('found text: ', text)\n",
        "                best_text = text\n",
        "                break\n",
        "            else:\n",
        "                print('not in dict: ', text)\n",
        "        return best_text\n",
        "\n",
        "def applyLM(parentBeam, childBeam, classes, lm):\n",
        "    \"calculate LM score of child beam by taking score from parent beam and bigram probability of last two chars\"\n",
        "    if lm and not childBeam.lmApplied:\n",
        "        c1 = classes[parentBeam.labeling[-1] if parentBeam.labeling else classes.index(' ')] # first char\n",
        "        c2 = classes[childBeam.labeling[-1]] # second char\n",
        "        lmFactor = 0.01 # influence of language model\n",
        "        bigramProb = lm.getCharBigram(c1, c2) ** lmFactor # probability of seeing first and second char next to each other\n",
        "        childBeam.prText = parentBeam.prText * bigramProb # probability of char sequence\n",
        "        childBeam.lmApplied = True # only apply LM once per beam entry\n",
        "\n",
        "def addBeam(beamState, labeling):\n",
        "    \"add beam if it does not yet exist\"\n",
        "    if labeling not in beamState.entries:\n",
        "        beamState.entries[labeling] = BeamEntry()\n",
        "\n",
        "def ctcBeamSearch(mat, classes, ignore_idx, lm, beamWidth=25, dict_list = []):\n",
        "    \"beam search as described by the paper of Hwang et al. and the paper of Graves et al.\"\n",
        "\n",
        "    #blankIdx = len(classes)\n",
        "    blankIdx = 0\n",
        "    maxT, maxC = mat.shape\n",
        "\n",
        "    # initialise beam state\n",
        "    last = BeamState()\n",
        "    labeling = ()\n",
        "    last.entries[labeling] = BeamEntry()\n",
        "    last.entries[labeling].prBlank = 1\n",
        "    last.entries[labeling].prTotal = 1\n",
        "\n",
        "    # go over all time-steps\n",
        "    for t in range(maxT):\n",
        "        curr = BeamState()\n",
        "\n",
        "        # get beam-labelings of best beams\n",
        "        bestLabelings = last.sort()[0:beamWidth]\n",
        "\n",
        "        # go over best beams\n",
        "        for labeling in bestLabelings:\n",
        "\n",
        "            # probability of paths ending with a non-blank\n",
        "            prNonBlank = 0\n",
        "            # in case of non-empty beam\n",
        "            if labeling:\n",
        "                # probability of paths with repeated last char at the end\n",
        "                prNonBlank = last.entries[labeling].prNonBlank * mat[t, labeling[-1]]\n",
        "\n",
        "            # probability of paths ending with a blank\n",
        "            prBlank = (last.entries[labeling].prTotal) * mat[t, blankIdx]\n",
        "\n",
        "            # add beam at current time-step if needed\n",
        "            addBeam(curr, labeling)\n",
        "\n",
        "            # fill in data\n",
        "            curr.entries[labeling].labeling = labeling\n",
        "            curr.entries[labeling].prNonBlank += prNonBlank\n",
        "            curr.entries[labeling].prBlank += prBlank\n",
        "            curr.entries[labeling].prTotal += prBlank + prNonBlank\n",
        "            curr.entries[labeling].prText = last.entries[labeling].prText # beam-labeling not changed, therefore also LM score unchanged from\n",
        "            curr.entries[labeling].lmApplied = True # LM already applied at previous time-step for this beam-labeling\n",
        "\n",
        "            # extend current beam-labeling\n",
        "            for c in range(maxC - 1):\n",
        "                # add new char to current beam-labeling\n",
        "                newLabeling = labeling + (c,)\n",
        "\n",
        "                # if new labeling contains duplicate char at the end, only consider paths ending with a blank\n",
        "                if labeling and labeling[-1] == c:\n",
        "                    prNonBlank = mat[t, c] * last.entries[labeling].prBlank\n",
        "                else:\n",
        "                    prNonBlank = mat[t, c] * last.entries[labeling].prTotal\n",
        "\n",
        "                # add beam at current time-step if needed\n",
        "                addBeam(curr, newLabeling)\n",
        "\n",
        "                # fill in data\n",
        "                curr.entries[newLabeling].labeling = newLabeling\n",
        "                curr.entries[newLabeling].prNonBlank += prNonBlank\n",
        "                curr.entries[newLabeling].prTotal += prNonBlank\n",
        "\n",
        "                # apply LM\n",
        "                #applyLM(curr.entries[labeling], curr.entries[newLabeling], classes, lm)\n",
        "\n",
        "        # set new beam state\n",
        "        last = curr\n",
        "\n",
        "    # normalise LM scores according to beam-labeling-length\n",
        "    last.norm()\n",
        "\n",
        "    # sort by probability\n",
        "    #bestLabeling = last.sort()[0] # get most probable labeling\n",
        "\n",
        "    # map labels to chars\n",
        "    #res = ''\n",
        "    #for idx,l in enumerate(bestLabeling):\n",
        "    #    if l not in ignore_idx and (not (idx > 0 and bestLabeling[idx - 1] == bestLabeling[idx])):  # removing repeated characters and blank.\n",
        "    #        res += classes[l]\n",
        "\n",
        "    if dict_list == []:\n",
        "        bestLabeling = last.sort()[0] # get most probable labeling\n",
        "        res = ''\n",
        "        for i,l in enumerate(bestLabeling):\n",
        "            if l not in ignore_idx and (not (i > 0 and bestLabeling[i - 1] == bestLabeling[i])):  # removing repeated characters and blank.\n",
        "                res += classes[l]\n",
        "    else:\n",
        "        res = last.wordsearch(classes, ignore_idx, beamWidth, dict_list)\n",
        "\n",
        "    return res\n",
        "#####\n",
        "\n",
        "def consecutive(data, mode ='first', stepsize=1):\n",
        "    group = np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
        "    group = [item for item in group if len(item)>0]\n",
        "\n",
        "    if mode == 'first': result = [l[0] for l in group]\n",
        "    elif mode == 'last': result = [l[-1] for l in group]\n",
        "    return result\n",
        "\n",
        "def word_segmentation(mat, separator_idx =  {'th': [1,2],'en': [3,4]}, separator_idx_list = [1,2,3,4]):\n",
        "    result = []\n",
        "    sep_list = []\n",
        "    start_idx = 0\n",
        "    for sep_idx in separator_idx_list:\n",
        "        if sep_idx % 2 == 0: mode ='first'\n",
        "        else: mode ='last'\n",
        "        a = consecutive( np.argwhere(mat == sep_idx).flatten(), mode)\n",
        "        new_sep = [ [item, sep_idx] for item in a]\n",
        "        sep_list += new_sep\n",
        "    sep_list = sorted(sep_list, key=lambda x: x[0])\n",
        "\n",
        "    for sep in sep_list:\n",
        "        for lang in separator_idx.keys():\n",
        "            if sep[1] == separator_idx[lang][0]: # start lang\n",
        "                sep_lang = lang\n",
        "                sep_start_idx = sep[0]\n",
        "            elif sep[1] == separator_idx[lang][1]: # end lang\n",
        "                if sep_lang == lang: # check if last entry if the same start lang\n",
        "                    new_sep_pair = [lang, [sep_start_idx+1, sep[0]-1]]\n",
        "                    if sep_start_idx > start_idx:\n",
        "                        result.append( ['', [start_idx, sep_start_idx-1] ] )\n",
        "                    start_idx = sep[0]+1\n",
        "                    result.append(new_sep_pair)\n",
        "                else: # reset\n",
        "                    sep_lang = ''\n",
        "\n",
        "    if start_idx <= len(mat)-1:\n",
        "        result.append( ['', [start_idx, len(mat)-1] ] )\n",
        "    return result\n",
        "\n",
        "class CTCLabelConverter(object):\n",
        "    \"\"\" Convert between text-label and text-index \"\"\"\n",
        "\n",
        "    #def __init__(self, character, separator = []):\n",
        "    def __init__(self, character, separator_list = {}, dict_pathlist = {}):\n",
        "        # character (str): set of the possible characters.\n",
        "        dict_character = list(character)\n",
        "\n",
        "        #special_character = ['\\xa2', '\\xa3', '\\xa4','\\xa5']\n",
        "        #self.separator_char = special_character[:len(separator)]\n",
        "\n",
        "        self.dict = {}\n",
        "        #for i, char in enumerate(self.separator_char + dict_character):\n",
        "        for i, char in enumerate(dict_character):\n",
        "            # NOTE: 0 is reserved for 'blank' token required by CTCLoss\n",
        "            self.dict[char] = i + 1\n",
        "\n",
        "        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
        "        #self.character = ['[blank]']+ self.separator_char + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
        "        self.separator_list = separator_list\n",
        "\n",
        "        separator_char = []\n",
        "        for lang, sep in separator_list.items():\n",
        "            separator_char += sep\n",
        "\n",
        "        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]\n",
        "\n",
        "        dict_list = {}\n",
        "        for lang, dict_path in dict_pathlist.items():\n",
        "            with open(dict_path, \"rb\") as input_file:\n",
        "                word_count = pickle.load(input_file)\n",
        "            dict_list[lang] = word_count\n",
        "        self.dict_list = dict_list\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\"convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "\n",
        "        output:\n",
        "            text: concatenated text index for CTCLoss.\n",
        "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
        "            length: length of each text. [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) for s in text]\n",
        "        text = ''.join(text)\n",
        "        text = [self.dict[char] for char in text]\n",
        "\n",
        "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
        "\n",
        "    def decode_greedy(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        index = 0\n",
        "        for l in length:\n",
        "            t = text_index[index:index + l]\n",
        "\n",
        "            char_list = []\n",
        "            for i in range(l):\n",
        "                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
        "                #if (t[i] != 0) and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
        "                    char_list.append(self.character[t[i]])\n",
        "            text = ''.join(char_list)\n",
        "\n",
        "            texts.append(text)\n",
        "            index += l\n",
        "        return texts\n",
        "\n",
        "    def decode_beamsearch(self, mat, beamWidth=5):\n",
        "        texts = []\n",
        "\n",
        "        for i in range(mat.shape[0]):\n",
        "            t = ctcBeamSearch(mat[i], self.character, self.ignore_idx, None, beamWidth=beamWidth)\n",
        "            texts.append(t)\n",
        "        return texts\n",
        "\n",
        "    def decode_wordbeamsearch(self, mat, beamWidth=5):\n",
        "        texts = []\n",
        "        argmax = np.argmax(mat, axis = 2)\n",
        "        for i in range(mat.shape[0]):\n",
        "            words = word_segmentation(argmax[i])\n",
        "            string = ''\n",
        "            for word in words:\n",
        "                matrix = mat[i, word[1][0]:word[1][1]+1,:]\n",
        "                if word[0] == '': dict_list = []\n",
        "                else: dict_list = self.dict_list[word[0]]\n",
        "                t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None, beamWidth=beamWidth, dict_list=dict_list)\n",
        "                string += t\n",
        "            texts.append(string)\n",
        "        return texts\n",
        "\n",
        "class AttnLabelConverter(object):\n",
        "    \"\"\" Convert between text-label and text-index \"\"\"\n",
        "\n",
        "    def __init__(self, character):\n",
        "        # character (str): set of the possible characters.\n",
        "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
        "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
        "        list_character = list(character)\n",
        "        self.character = list_token + list_character\n",
        "\n",
        "        self.dict = {}\n",
        "        for i, char in enumerate(self.character):\n",
        "            # print(i, char)\n",
        "            self.dict[char] = i\n",
        "\n",
        "    def encode(self, text, batch_max_length=25):\n",
        "        \"\"\" convert text-label into text-index.\n",
        "        input:\n",
        "            text: text labels of each image. [batch_size]\n",
        "            batch_max_length: max length of text label in the batch. 25 by default\n",
        "\n",
        "        output:\n",
        "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
        "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
        "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
        "        \"\"\"\n",
        "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
        "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
        "        batch_max_length += 1\n",
        "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
        "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
        "        for i, t in enumerate(text):\n",
        "            text = list(t)\n",
        "            text.append('[s]')\n",
        "            text = [self.dict[char] for char in text]\n",
        "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
        "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
        "\n",
        "    def decode(self, text_index, length):\n",
        "        \"\"\" convert text-index into text-label. \"\"\"\n",
        "        texts = []\n",
        "        for index, l in enumerate(length):\n",
        "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
        "            texts.append(text)\n",
        "        return texts\n",
        "\n",
        "\n",
        "class Averager(object):\n",
        "    \"\"\"Compute average for torch.Tensor, used for loss average.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def add(self, v):\n",
        "        count = v.data.numel()\n",
        "        v = v.data.sum()\n",
        "        self.n_count += count\n",
        "        self.sum += v\n",
        "\n",
        "    def reset(self):\n",
        "        self.n_count = 0\n",
        "        self.sum = 0\n",
        "\n",
        "    def val(self):\n",
        "        res = 0\n",
        "        if self.n_count != 0:\n",
        "            res = self.sum / float(self.n_count)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tuLCTkamTJIo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(Model, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.stages = {'Trans': opt.Transformation, 'Feat': opt.FeatureExtraction,\n",
        "                       'Seq': opt.SequenceModeling, 'Pred': opt.Prediction}\n",
        "\n",
        "        \"\"\" Transformation \"\"\"\n",
        "        if opt.Transformation == 'TPS':\n",
        "            self.Transformation = TPS_SpatialTransformerNetwork(\n",
        "                F=opt.num_fiducial, I_size=(opt.imgH, opt.imgW), I_r_size=(opt.imgH, opt.imgW), I_channel_num=opt.input_channel)\n",
        "        else:\n",
        "            print('No Transformation module specified')\n",
        "\n",
        "        \"\"\" FeatureExtraction \"\"\"\n",
        "        if opt.FeatureExtraction == 'VGG':\n",
        "            self.FeatureExtraction = VGG_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        elif opt.FeatureExtraction == 'RCNN':\n",
        "            self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        elif opt.FeatureExtraction == 'ResNet':\n",
        "            self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
        "        else:\n",
        "            raise Exception('No FeatureExtraction module specified')\n",
        "        self.FeatureExtraction_output = opt.output_channel  # int(imgH/16-1) * 512\n",
        "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
        "\n",
        "        \"\"\" Sequence modeling\"\"\"\n",
        "        if opt.SequenceModeling == 'BiLSTM':\n",
        "            self.SequenceModeling = nn.Sequential(\n",
        "                BidirectionalLSTM(self.FeatureExtraction_output, opt.hidden_size, opt.hidden_size),\n",
        "                BidirectionalLSTM(opt.hidden_size, opt.hidden_size, opt.hidden_size))\n",
        "            self.SequenceModeling_output = opt.hidden_size\n",
        "        else:\n",
        "            print('No SequenceModeling module specified')\n",
        "            self.SequenceModeling_output = self.FeatureExtraction_output\n",
        "\n",
        "        \"\"\" Prediction \"\"\"\n",
        "        if opt.Prediction == 'CTC':\n",
        "            self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)\n",
        "        elif opt.Prediction == 'Attn':\n",
        "            self.Prediction = Attention(self.SequenceModeling_output, opt.hidden_size, opt.num_class)\n",
        "        else:\n",
        "            raise Exception('Prediction is neither CTC or Attn')\n",
        "\n",
        "    def forward(self, input, text, is_train=True):\n",
        "        \"\"\" Transformation stage \"\"\"\n",
        "        if not self.stages['Trans'] == \"None\":\n",
        "            input = self.Transformation(input)\n",
        "\n",
        "        \"\"\" Feature extraction stage \"\"\"\n",
        "        visual_feature = self.FeatureExtraction(input)\n",
        "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
        "        visual_feature = visual_feature.squeeze(3)\n",
        "\n",
        "        \"\"\" Sequence modeling stage \"\"\"\n",
        "        if self.stages['Seq'] == 'BiLSTM':\n",
        "            contextual_feature = self.SequenceModeling(visual_feature)\n",
        "        else:\n",
        "            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM\n",
        "\n",
        "        \"\"\" Prediction stage \"\"\"\n",
        "        if self.stages['Pred'] == 'CTC':\n",
        "            prediction = self.Prediction(contextual_feature.contiguous())\n",
        "        else:\n",
        "            prediction = self.Prediction(contextual_feature.contiguous(), text, is_train, batch_max_length=self.opt.batch_max_length)\n",
        "\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Z05luqvQMoG7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import string\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "\n",
        "def validation(model, criterion, evaluation_loader, converter, opt, device):\n",
        "    \"\"\" validation or evaluation \"\"\"\n",
        "    n_correct = 0\n",
        "    norm_ED = 0\n",
        "    length_of_data = 0\n",
        "    infer_time = 0\n",
        "    valid_loss_avg = Averager()\n",
        "\n",
        "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
        "        batch_size = image_tensors.size(0)\n",
        "        length_of_data = length_of_data + batch_size\n",
        "        image = image_tensors.to(device)\n",
        "        # For max length prediction\n",
        "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
        "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
        "\n",
        "        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
        "\n",
        "        start_time = time.time()\n",
        "        if 'CTC' in opt.Prediction:\n",
        "            preds = model(image, text_for_pred)\n",
        "            forward_time = time.time() - start_time\n",
        "\n",
        "            # Calculate evaluation loss for CTC decoder.\n",
        "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "            # permute 'preds' to use CTCloss format\n",
        "            cost = criterion(preds.log_softmax(2).permute(1, 0, 2), text_for_loss, preds_size, length_for_loss)\n",
        "\n",
        "            if opt.decode == 'greedy':\n",
        "                # Select max probabilty (greedy decoding) then decode index to character\n",
        "                _, preds_index = preds.max(2)\n",
        "                preds_index = preds_index.view(-1)\n",
        "                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
        "            elif opt.decode == 'beamsearch':\n",
        "                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n",
        "\n",
        "        else:\n",
        "            preds = model(image, text_for_pred, is_train=False)\n",
        "            forward_time = time.time() - start_time\n",
        "\n",
        "            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n",
        "            target = text_for_loss[:, 1:]  # without [GO] Symbol\n",
        "            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
        "\n",
        "            # select max probabilty (greedy decoding) then decode index to character\n",
        "            _, preds_index = preds.max(2)\n",
        "            preds_str = converter.decode(preds_index, length_for_pred)\n",
        "            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n",
        "\n",
        "        infer_time += forward_time\n",
        "        valid_loss_avg.add(cost)\n",
        "\n",
        "        # calculate accuracy & confidence score\n",
        "        preds_prob = F.softmax(preds, dim=2)\n",
        "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
        "        confidence_score_list = []\n",
        "\n",
        "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
        "            if 'Attn' in opt.Prediction:\n",
        "                gt = gt[:gt.find('[s]')]\n",
        "                pred_EOS = pred.find('[s]')\n",
        "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
        "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
        "\n",
        "            if pred == gt:\n",
        "                n_correct += 1\n",
        "\n",
        "            '''\n",
        "            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n",
        "            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\"\n",
        "            if len(gt) == 0:\n",
        "                norm_ED += 1\n",
        "            else:\n",
        "                norm_ED += edit_distance(pred, gt) / len(gt)\n",
        "            '''\n",
        "\n",
        "            # ICDAR2019 Normalized Edit Distance\n",
        "            if len(gt) == 0 or len(pred) ==0:\n",
        "                norm_ED += 0\n",
        "            elif len(gt) > len(pred):\n",
        "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
        "            else:\n",
        "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
        "\n",
        "            # calculate confidence score (= multiply of pred_max_prob)\n",
        "            try:\n",
        "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
        "            except:\n",
        "                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n",
        "            confidence_score_list.append(confidence_score)\n",
        "            # print(pred, gt, pred==gt, confidence_score)\n",
        "\n",
        "    accuracy = n_correct / float(length_of_data) * 100\n",
        "    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n",
        "\n",
        "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eIuUomvqJBsj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def count_parameters(model):\n",
        "    print(\"Modules, Parameters\")\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        #table.add_row([name, param])\n",
        "        total_params+=param\n",
        "        print(name, param)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "def train(opt, show_number = 2, amp=False):\n",
        "    \"\"\" dataset preparation \"\"\"\n",
        "    if not opt.data_filtering_off:\n",
        "        print('Filtering the images containing characters which are not in opt.character')\n",
        "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
        "\n",
        "    opt.select_data = opt.select_data.split('-')\n",
        "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
        "    train_dataset = Batch_Balanced_Dataset(opt)\n",
        "\n",
        "    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n",
        "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n",
        "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=min(32, opt.batch_size),\n",
        "        shuffle=True,  # 'True' to check training progress with validation function.\n",
        "        num_workers=int(opt.workers), prefetch_factor=512,\n",
        "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
        "    log.write(valid_dataset_log)\n",
        "    print('-' * 80)\n",
        "    log.write('-' * 80 + '\\n')\n",
        "    log.close()\n",
        "\n",
        "    \"\"\" model configuration \"\"\"\n",
        "    if 'CTC' in opt.Prediction:\n",
        "        converter = CTCLabelConverter(opt.character)\n",
        "    else:\n",
        "        converter = AttnLabelConverter(opt.character)\n",
        "    opt.num_class = len(converter.character)\n",
        "\n",
        "    if opt.rgb:\n",
        "        opt.input_channel = 3\n",
        "    model = Model(opt)\n",
        "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
        "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
        "          opt.SequenceModeling, opt.Prediction)\n",
        "\n",
        "    if opt.saved_model != '':\n",
        "        pretrained_dict = torch.load(opt.saved_model,map_location=torch.device('cpu'))\n",
        "        if opt.new_prediction:\n",
        "            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))\n",
        "\n",
        "        model = torch.nn.DataParallel(model).to(device)\n",
        "        print(f'loading pretrained model from {opt.saved_model}')\n",
        "        if opt.FT:\n",
        "            model.load_state_dict(pretrained_dict, strict=False)\n",
        "        else:\n",
        "            model.load_state_dict(pretrained_dict)\n",
        "        if opt.new_prediction:\n",
        "            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)\n",
        "            for name, param in model.module.Prediction.named_parameters():\n",
        "                if 'bias' in name:\n",
        "                    init.constant_(param, 0.0)\n",
        "                elif 'weight' in name:\n",
        "                    init.kaiming_normal_(param)\n",
        "            model = model.to(device)\n",
        "    else:\n",
        "        # weight initialization\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'localization_fc2' in name:\n",
        "                print(f'Skip {name} as it is already initialized')\n",
        "                continue\n",
        "            try:\n",
        "                if 'bias' in name:\n",
        "                    init.constant_(param, 0.0)\n",
        "                elif 'weight' in name:\n",
        "                    init.kaiming_normal_(param)\n",
        "            except Exception as e:  # for batchnorm.\n",
        "                if 'weight' in name:\n",
        "                    param.data.fill_(1)\n",
        "                continue\n",
        "        model = torch.nn.DataParallel(model).to(device)\n",
        "\n",
        "    model.train()\n",
        "    print(\"Model:\")\n",
        "    print(model)\n",
        "    count_parameters(model)\n",
        "\n",
        "    \"\"\" setup loss \"\"\"\n",
        "    if 'CTC' in opt.Prediction:\n",
        "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
        "    else:\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
        "    # loss averager\n",
        "    loss_avg = Averager()\n",
        "\n",
        "    # freeze some layers\n",
        "    try:\n",
        "        if opt.freeze_FeatureFxtraction:\n",
        "            for param in model.module.FeatureExtraction.parameters():\n",
        "                param.requires_grad = False\n",
        "        if opt.freeze_SequenceModeling:\n",
        "            for param in model.module.SequenceModeling.parameters():\n",
        "                param.requires_grad = False\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # filter that only require gradient decent\n",
        "    filtered_parameters = []\n",
        "    params_num = []\n",
        "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
        "        filtered_parameters.append(p)\n",
        "        params_num.append(np.prod(p.size()))\n",
        "    print('Trainable params num : ', sum(params_num))\n",
        "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
        "\n",
        "    # setup optimizer\n",
        "    if opt.optim=='adam':\n",
        "        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "        optimizer = optim.Adam(filtered_parameters)\n",
        "    else:\n",
        "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
        "    print(\"Optimizer:\")\n",
        "    print(optimizer)\n",
        "\n",
        "    \"\"\" final options \"\"\"\n",
        "    # print(opt)\n",
        "    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n",
        "        opt_log = '------------ Options -------------\\n'\n",
        "        args = vars(opt)\n",
        "        for k, v in args.items():\n",
        "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
        "        opt_log += '---------------------------------------\\n'\n",
        "        print(opt_log)\n",
        "        opt_file.write(opt_log)\n",
        "\n",
        "    \"\"\" start training \"\"\"\n",
        "    start_iter = 0\n",
        "    if opt.saved_model != '':\n",
        "        try:\n",
        "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
        "            print(f'continue to train, start_iter: {start_iter}')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    start_time = time.time()\n",
        "    best_accuracy = -1\n",
        "    best_norm_ED = -1\n",
        "    i = start_iter\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    t1= time.time()\n",
        "\n",
        "    while(True):\n",
        "        # train part\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if amp:\n",
        "            with autocast():\n",
        "                image_tensors, labels = train_dataset.get_batch()\n",
        "                image = image_tensors.to(device)\n",
        "                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
        "                batch_size = image.size(0)\n",
        "\n",
        "                if 'CTC' in opt.Prediction:\n",
        "                    preds = model(image, text).log_softmax(2)\n",
        "                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "                    preds = preds.permute(1, 0, 2)\n",
        "                    torch.backends.cudnn.enabled = False\n",
        "                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
        "                    torch.backends.cudnn.enabled = True\n",
        "                else:\n",
        "                    preds = model(image, text[:, :-1])  # align with Attention.forward\n",
        "                    target = text[:, 1:]  # without [GO] Symbol\n",
        "                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
        "            scaler.scale(cost).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            image_tensors, labels = train_dataset.get_batch()\n",
        "            image = image_tensors.to(device)\n",
        "            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
        "            batch_size = image.size(0)\n",
        "            if 'CTC' in opt.Prediction:\n",
        "                preds = model(image, text).log_softmax(2)\n",
        "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "                preds = preds.permute(1, 0, 2)\n",
        "                torch.backends.cudnn.enabled = False\n",
        "                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
        "                torch.backends.cudnn.enabled = True\n",
        "            else:\n",
        "                preds = model(image, text[:, :-1])  # align with Attention.forward\n",
        "                target = text[:, 1:]  # without [GO] Symbol\n",
        "                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
        "            cost.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
        "            optimizer.step()\n",
        "        loss_avg.add(cost)\n",
        "\n",
        "        # validation part\n",
        "        if (i % opt.valInterval == 0) and (i!=0):\n",
        "            print('training time: ', time.time()-t1)\n",
        "            t1=time.time()\n",
        "            elapsed_time = time.time() - start_time\n",
        "            # for log\n",
        "            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n",
        "                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n",
        "                model.train()\n",
        "\n",
        "                # training loss and validation loss\n",
        "                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
        "                loss_avg.reset()\n",
        "\n",
        "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n",
        "\n",
        "                # keep best accuracy model (on valid dataset)\n",
        "                if current_accuracy > best_accuracy:\n",
        "                    best_accuracy = current_accuracy\n",
        "                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_accuracy.pth')\n",
        "                if current_norm_ED > best_norm_ED:\n",
        "                    best_norm_ED = current_norm_ED\n",
        "                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_norm_ED.pth')\n",
        "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n",
        "\n",
        "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
        "                print(loss_model_log)\n",
        "                log.write(loss_model_log + '\\n')\n",
        "\n",
        "                # show some predicted results\n",
        "                dashed_line = '-' * 80\n",
        "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
        "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
        "\n",
        "                #show_number = min(show_number, len(labels))\n",
        "\n",
        "                start = random.randint(0,len(labels) - show_number )\n",
        "                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n",
        "                    if 'Attn' in opt.Prediction:\n",
        "                        gt = gt[:gt.find('[s]')]\n",
        "                        pred = pred[:pred.find('[s]')]\n",
        "\n",
        "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
        "                predicted_result_log += f'{dashed_line}'\n",
        "                print(predicted_result_log)\n",
        "                log.write(predicted_result_log + '\\n')\n",
        "                print('validation time: ', time.time()-t1)\n",
        "                t1=time.time()\n",
        "        # save model per 1e+4 iter.\n",
        "        if (i + 1) % 1e+4 == 0:\n",
        "            torch.save(\n",
        "                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n",
        "\n",
        "        if i == opt.num_iter:\n",
        "            print('end the training')\n",
        "            sys.exit()\n",
        "        i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2ZW52fO-FTsi"
      },
      "outputs": [],
      "source": [
        "def get_config(file_path):\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
        "        opt = yaml.safe_load(stream)\n",
        "    opt = AttrDict(opt)\n",
        "    if opt.lang_char == 'None':\n",
        "        characters = ''\n",
        "        for data in opt['select_data'].split('-'):\n",
        "            csv_path = os.path.join(opt['train_data'], data, 'label.csv')\n",
        "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'text'], keep_default_na=False)\n",
        "            all_char = ''.join(df['text'])\n",
        "            characters += ''.join(set(all_char))\n",
        "        characters = sorted(set(characters))\n",
        "        opt.character= ''.join(characters)\n",
        "    else:\n",
        "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
        "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
        "    return opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kIeSJtRdYex8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e99e7fe-81f8-448c-f126-fd8b153ea4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmdir: failed to remove '/content/all_data/.ipynb_checkpoints': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "rmdir /content/all_data/.ipynb_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAawwCJIFTsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb4234a-63b3-4731-e291-d4bc9787d9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering the images containing characters which are not in opt.character\n",
            "Filtering the images whose label is longer than opt.batch_max_length\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root: all_data\n",
            "opt.select_data: ['all_data']\n",
            "opt.batch_ratio: ['1']\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    all_data\t dataset: all_data\n",
            "all_data/en_sample\n",
            "sub-directory:\t/en_sample\t num samples: 4137\n",
            "num total samples of all_data: 4137 x 1.0 (total_data_usage_ratio) = 4137\n",
            "num samples of all_data per batch: 32 x 1.0 (batch_ratio) = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Total_batch_size: 32 = 32\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    all_data/en_sample\t dataset: /\n",
            "all_data/en_sample/\n",
            "sub-directory:\t/.\t num samples: 4137\n",
            "--------------------------------------------------------------------------------\n",
            "No Transformation module specified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model input parameters 64 600 20 1 256 256 97 34 None VGG BiLSTM CTC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-2f94be91297c>:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_dict = torch.load(opt.saved_model,map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading pretrained model from saved_models/english_g2.pth\n",
            "Model:\n",
            "DataParallel(\n",
            "  (module): Model(\n",
            "    (FeatureExtraction): VGG_FeatureExtractor(\n",
            "      (ConvNet): Sequential(\n",
            "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): ReLU(inplace=True)\n",
            "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (7): ReLU(inplace=True)\n",
            "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (9): ReLU(inplace=True)\n",
            "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (13): ReLU(inplace=True)\n",
            "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (16): ReLU(inplace=True)\n",
            "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (19): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
            "    (SequenceModeling): Sequential(\n",
            "      (0): BidirectionalLSTM(\n",
            "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "      (1): BidirectionalLSTM(\n",
            "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (Prediction): Linear(in_features=256, out_features=97, bias=True)\n",
            "  )\n",
            ")\n",
            "Modules, Parameters\n",
            "module.FeatureExtraction.ConvNet.0.weight 288\n",
            "module.FeatureExtraction.ConvNet.0.bias 32\n",
            "module.FeatureExtraction.ConvNet.3.weight 18432\n",
            "module.FeatureExtraction.ConvNet.3.bias 64\n",
            "module.FeatureExtraction.ConvNet.6.weight 73728\n",
            "module.FeatureExtraction.ConvNet.6.bias 128\n",
            "module.FeatureExtraction.ConvNet.8.weight 147456\n",
            "module.FeatureExtraction.ConvNet.8.bias 128\n",
            "module.FeatureExtraction.ConvNet.11.weight 294912\n",
            "module.FeatureExtraction.ConvNet.12.weight 256\n",
            "module.FeatureExtraction.ConvNet.12.bias 256\n",
            "module.FeatureExtraction.ConvNet.14.weight 589824\n",
            "module.FeatureExtraction.ConvNet.15.weight 256\n",
            "module.FeatureExtraction.ConvNet.15.bias 256\n",
            "module.FeatureExtraction.ConvNet.18.weight 262144\n",
            "module.FeatureExtraction.ConvNet.18.bias 256\n",
            "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
            "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
            "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
            "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
            "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
            "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
            "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
            "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
            "module.SequenceModeling.0.linear.weight 131072\n",
            "module.SequenceModeling.0.linear.bias 256\n",
            "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
            "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
            "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
            "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
            "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
            "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
            "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
            "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
            "module.SequenceModeling.1.linear.weight 131072\n",
            "module.SequenceModeling.1.linear.bias 256\n",
            "module.Prediction.weight 24832\n",
            "module.Prediction.bias 97\n",
            "Total Trainable Params: 3781345\n",
            "Trainable params num :  3781345\n",
            "Optimizer:\n",
            "Adadelta (\n",
            "Parameter Group 0\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 1.0\n",
            "    maximize: False\n",
            "    rho: 0.95\n",
            "    weight_decay: 0\n",
            ")\n",
            "------------ Options -------------\n",
            "number: 0123456789\n",
            "symbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €\n",
            "lang_char: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "experiment_name: en_filtered\n",
            "train_data: all_data\n",
            "valid_data: all_data/en_sample\n",
            "manualSeed: 1111\n",
            "workers: 6\n",
            "batch_size: 32\n",
            "num_iter: 300000\n",
            "valInterval: 20000\n",
            "saved_model: saved_models/english_g2.pth\n",
            "FT: False\n",
            "optim: False\n",
            "lr: 1.0\n",
            "beta1: 0.9\n",
            "rho: 0.95\n",
            "eps: 1e-08\n",
            "grad_clip: 5\n",
            "select_data: ['all_data']\n",
            "batch_ratio: ['1']\n",
            "total_data_usage_ratio: 1.0\n",
            "batch_max_length: 34\n",
            "imgH: 64\n",
            "imgW: 600\n",
            "rgb: False\n",
            "contrast_adjust: 0.0\n",
            "sensitive: True\n",
            "PAD: True\n",
            "data_filtering_off: False\n",
            "Transformation: None\n",
            "FeatureExtraction: VGG\n",
            "SequenceModeling: BiLSTM\n",
            "Prediction: CTC\n",
            "num_fiducial: 20\n",
            "input_channel: 1\n",
            "output_channel: 256\n",
            "hidden_size: 256\n",
            "decode: greedy\n",
            "new_prediction: False\n",
            "freeze_FeatureFxtraction: False\n",
            "freeze_SequenceModeling: False\n",
            "character: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "num_class: 97\n",
            "---------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-2f94be91297c>:168: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time:  2982.6537873744965\n",
            "[20000/300000] Train loss: 0.00573, Valid loss: 0.00010, Elapsed_time: 2982.65640\n",
            "Current_accuracy : 99.952, Current_norm_ED  : 0.9999\n",
            "Best_accuracy    : 99.952, Best_norm_ED     : 0.9999\n",
            "--------------------------------------------------------------------------------\n",
            "Ground Truth              | Prediction                | Confidence Score & T/F\n",
            "--------------------------------------------------------------------------------\n",
            "BP8118B                   | BP8118B                   | 0.9984\tTrue\n",
            "E6758QZ                   | E6758QZ                   | 0.5680\tTrue\n",
            "--------------------------------------------------------------------------------\n",
            "validation time:  44.75363302230835\n"
          ]
        }
      ],
      "source": [
        "opt = get_config(\"config_files/en_fine_tunning_config.yaml\")\n",
        "train(opt, amp=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkR_2bPhq6kZ"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), './saved_models/easyOCR_v1.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}